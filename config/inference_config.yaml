max_len: 256
logging_level: INFO

archs:
  selected_model: gpt
  rnn:
    # RNN | LSTM | GRU
    block: LSTM
    embedding_size: 512
    hidden_size: 512
    num_layers: 5
    block_dropout: .15
    bidirectional: False
    model_path: models/final/lstm_512.pt
    tokenizer_path: data/tokenizers/gpt/tokenizer_gpt.json

    sampler:
      nucleus:
        p: 0.95
        temperature: 0.65

  gpt:
    n_layers: 10
    embedding_size: 512

    positional_encoder:
      type: learned
      max_length: 256
      dropout: 0.15

    multihead:
      n_heads: 8
      dropout: 0.1

      attention: 
        size: 512
        dropout: 0.1

    feadforward:
      dropout: 0.2

    model_path: models/final/anek_gpt.pt
    tokenizer_path: data/tokenizers/gpt/tokenizer_gpt.json

    sampler:
      nucleus:
        p: 0.95
        temperature: 0.7

  gpt_lora:
    model_path: models/final/gpt_medium
    model_path_hf: 'ai-forever/rugpt3medium_based_on_gpt2'
    lora_path: models/final/gpt_medium/lora
    tokenizer_path: data/tokenizers/gpt_lora
    
    sampler:
      nucleus:
        p: 0.95
        temperature: 0.75
