# train|inference
device: cuda
logging_level: INFO

selected_model: gpt_lora
epoches: 150
make_checkpoint_every_n_epoch: 5
load_checkpoint_path: False #models/checkpoints/rnn_adamw_reducelronplateau/70_epoch.pt

amp:
  enabled: False
  scaler_init_scale: 256

optimizer:
  name: adamw
  lr: 1e-3
  weight_decay: 0.01

scheduler:
  # scheme: cosine_warmup
  # total_training_steps: 86055
  scheme: reducelronplateau
  factor: 0.1
  patience: 3
  cooldown: 3

archs:
  rnn:
    # RNN | LSTM | GRU
    block: LSTM
    embedding_size: 512
    hidden_size: 512
    num_layers: 5
    block_dropout: .15
    bidirectional: False
    tokenizer_path: data/tokenizers/gpt/tokenizer_gpt.json
  
  gpt:
    n_layers: 10
    embedding_size: 512

    positional_encoder:
      type: learned
      max_length: 256
      dropout: 0.15

    multihead:
      n_heads: 8
      dropout: 0.1

      attention: 
        size: 512
        dropout: 0.1

    feadforward:
      dropout: 0.2
    tokenizer_path: data/tokenizers/gpt/tokenizer_gpt.json

  gpt_lora:
    gradient_accumulation_steps: 10
    model_path: models/final/gpt_lora
    model_path_hf: 'ai-forever/rugpt3medium_based_on_gpt2'
    tokenizer_path: data/tokenizers/gpt_lora

    lora_config:
      r: 32
      target_modules: ['c_attn'] 
      lora_alpha: 64
      lora_dropout: 0.1
      bias: 'none'
      fan_in_fan_out: True
      task_type: "CAUSAL_LM"
      
data:
  batch_size: 32
  aneks_path: data/aneks/processed/for_lora/

save_checkpoint_path: models/checkpoints/

